{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "9288037e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6efdb188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "70d0fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# csv_path = r\"C:\\Users\\final\\FinancialTrackerApp\\model_code\\expenses_data.csv\"\n",
    "csv_path = r\"D:\\Programming\\Projects\\Finance-Tracker-App\\python_stuffs\\model_code\\expenses_data_cleaned.csv\"\n",
    "# BASE_DIR = os.path.join(os.getcwd(), \"model_code\")\n",
    "# csv_path = os.path.join(BASE_DIR, \"expenses_data.csv\")\n",
    "df = pd.read_csv(csv_path)\n",
    "texts = df['text']       \n",
    "labels = df['category']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "037baf86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(353, 560)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting text data into numerical vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e096513c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "       5])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encoding the labels\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(labels)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "17407cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train and test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert sparse matrices to dense arrays to avoid type errors\n",
    "X_train = X_train.toarray()\n",
    "X_test = X_test.toarray()\n",
    "\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1a576a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to(device)\n",
    "y_train = y_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_test = y_test.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "81a6b94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define model\n",
    "class ExpenseClassifier(nn.Module):\n",
    "    def __init__(self, input_size,hidden_size, num_classes):\n",
    "        super(ExpenseClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "25a120f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "560"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cebe98b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExpenseClassifier(\n",
       "  (fc1): Linear(in_features=560, out_features=64, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (fc2): Linear(in_features=64, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = X_train.shape[1]\n",
    "hidden_size = 64\n",
    "num_classes = len(set(y))\n",
    "\n",
    "model = ExpenseClassifier(input_size, hidden_size, num_classes)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "97d2cab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss() # compares predicted catgeory vs actual\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # updates model weights efficiently\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2a867f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/200], Loss: 1.1582\n",
      "Epoch [20/200], Loss: 0.1912\n",
      "Epoch [30/200], Loss: 0.0179\n",
      "Epoch [40/200], Loss: 0.0037\n",
      "Epoch [50/200], Loss: 0.0017\n",
      "Epoch [60/200], Loss: 0.0011\n",
      "Epoch [70/200], Loss: 0.0008\n",
      "Epoch [80/200], Loss: 0.0007\n",
      "Epoch [90/200], Loss: 0.0007\n",
      "Epoch [100/200], Loss: 0.0006\n",
      "Epoch [110/200], Loss: 0.0006\n",
      "Epoch [120/200], Loss: 0.0005\n",
      "Epoch [130/200], Loss: 0.0005\n",
      "Epoch [140/200], Loss: 0.0005\n",
      "Epoch [150/200], Loss: 0.0005\n",
      "Epoch [160/200], Loss: 0.0004\n",
      "Epoch [170/200], Loss: 0.0004\n",
      "Epoch [180/200], Loss: 0.0004\n",
      "Epoch [190/200], Loss: 0.0004\n",
      "Epoch [200/200], Loss: 0.0003\n"
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "epochs = 200\n",
    "for epoch in range(epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if(epoch+1)%10==0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f89ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8028\n"
     ]
    }
   ],
   "source": [
    "#Evaluation\n",
    "with torch.inference_mode():\n",
    "    y_pred = model(X_test) #forward pass\n",
    "    y_pred_classes = torch.argmax(y_pred, dim=1 ) #pick highest probability class\n",
    "    acc = (y_pred_classes == y_test).float().mean() #accuracy\n",
    "    print(f'Accuracy: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eaecbb30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vectorizer, model weights, and encoder saved\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "save_dir = \"pytorch_models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "joblib.dump(vectorizer, os.path.join(save_dir, \"vectorizer.pkl\"))\n",
    "\n",
    "# Save model weights\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, \"category_predictor_model.pth\"))\n",
    "\n",
    "# Save category mapping using PyTorch (from the LabelEncoder)\n",
    "category_list = list(encoder.classes_)  # this preserves exact order\n",
    "torch.save(category_list, os.path.join(save_dir, \"encoder.pth\"))\n",
    "\n",
    "print(\"✅ Vectorizer, model weights, and encoder saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "81cc9121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Transport': 450.0, 'Healthcare': 1250.0, 'Food': 80.0, 'Housing': 0.0, 'Education': 0.0, 'others': 500.0}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "totals = {\n",
    "    \"Transport\": 0.0,\n",
    "    \"Healthcare\": 0.0,\n",
    "    \"Food\": 0.0,\n",
    "    \"Housing\": 0.0,\n",
    "    \"Education\": 0.0,\n",
    "    \"others\": 0.0\n",
    "}\n",
    "\n",
    "def predict(text):\n",
    "    # Vectorize and move to correct device\n",
    "    vec = vectorizer.transform([text]).toarray()\n",
    "    vec = torch.tensor(vec, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Predict category\n",
    "    output = model(vec)\n",
    "    pred = torch.argmax(output, 1).item()\n",
    "    pred = encoder.inverse_transform([pred])[0]\n",
    "\n",
    "    # Extract all amounts\n",
    "    doc = nlp(text)\n",
    "    amount = 0.0\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in [\"MONEY\", \"CARDINAL\"]:\n",
    "            try:\n",
    "                amount += float(ent.text)  # sum all numbers in the sentence\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    totals[pred] += amount\n",
    "    # return totals\n",
    "# testing:\n",
    "predict(\"Bought apples for 80 rs\")   \n",
    "predict(\"Hospital bill as 750\")   \n",
    "predict(\"Taxi fare as 300\")       \n",
    "predict(\"Netflix subscription as 500\") \n",
    "predict(\"Train from Velachery as 150\") \n",
    "predict(\"Spent 500 at hotel\")\n",
    "\n",
    "print(totals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a49614f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700 CARDINAL\n",
      "300 CARDINAL\n",
      "26th July DATE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"I spent Rs. 700 and 300 on food on 26th July\")\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "320d8f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\Programming\\Projects\\Finance-Tracker-App\\python_stuffs\\model_code\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "027208df",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model_code\\\\pytorch_models\\\\encoder.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[94]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m encoder_path = os.path.join(\u001b[33m\"\u001b[39m\u001b[33mmodel_code\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpytorch_models\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mencoder.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m encoder = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(encoder.classes_)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Programming\\Projects\\Finance-Tracker-App\\python_stuffs\\venv\\Lib\\site-packages\\joblib\\numpy_pickle.py:735\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    733\u001b[39m         obj = _unpickle(fobj, ensure_native_byte_order=ensure_native_byte_order)\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m735\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    736\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m _validate_fileobject_and_memmap(f, filename, mmap_mode) \u001b[38;5;28;01mas\u001b[39;00m (\n\u001b[32m    737\u001b[39m             fobj,\n\u001b[32m    738\u001b[39m             validated_mmap_mode,\n\u001b[32m    739\u001b[39m         ):\n\u001b[32m    740\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fobj, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    741\u001b[39m                 \u001b[38;5;66;03m# if the returned file object is a string, this means we\u001b[39;00m\n\u001b[32m    742\u001b[39m                 \u001b[38;5;66;03m# try to load a pickle file generated with an version of\u001b[39;00m\n\u001b[32m    743\u001b[39m                 \u001b[38;5;66;03m# Joblib so we load it with joblib compatibility function.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'model_code\\\\pytorch_models\\\\encoder.pkl'"
     ]
    }
   ],
   "source": [
    "encoder_path = os.path.join(\"model_code\", \"pytorch_models\", \"encoder.pkl\")\n",
    "encoder = joblib.load(encoder_path)\n",
    "print(encoder.classes_)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
